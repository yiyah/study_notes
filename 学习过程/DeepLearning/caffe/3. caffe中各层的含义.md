# caffe

## 一、网络配置文件 --- train.prototxt

### 1.1 数据层

* 数据格式总共就三种：
    1. LMDB
    2. HDF
    3. 直接图片来源

* Q：一般都要对数据进行初始化的操作，那进行什么样的初始化操作呢？

  A：就是减去均值的操作。

* Q：这个均值是什么？

  A：这个均值是三个通道的均值。

* Q：怎么求？

  A：比如 N 个数据（图片），对所有的R,G,B通道求均值。

```protobuf
name: "CaffeNet" # 项目名字
layer {
  name: "data" # 该层名字,随便起
  type: "Data"
  top: "data" # 一般用bottom表示输入，top表示输出，多个top表示多个输出
  top: "label"
  include {
    phase: TRAIN # 只在训练用，训练网络分为训练阶段和自测试阶段，如果没include表示该层在测试中，又在训练中。
  }
  transform_param {
    mirror: true # 镜像，使数据变得更多，数据增强
    scale: 0.00390625 # (1/255)这是把原本的图片归一化到0~1。
    crop_size: 227 # 裁剪，数据增强，在输入的图裁剪一个范围227的图作为输入，数据又多了。
    mean_file: "data/ilsvrc12/imagenet_mean.binaryproto" # 均值文件
  }
# mean pixel / channel-wise mean instead of mean image
#  transform_param {
#    crop_size: 227
#    mean_value: 104
#    mean_value: 117
#    mean_value: 123
#    mirror: true
#  }
  data_param {
    source: "examples/imagenet/ilsvrc12_train_lmdb" # 数据来源
    batch_size: 256 # 每次批处理的个数
    backend: LMDB # 选用数据的类型
  }
}

layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "data/ilsvrc12/imagenet_mean.binaryproto"
  }
# mean pixel / channel-wise mean instead of mean image
#  transform_param {
#    crop_size: 227
#    mean_value: 104
#    mean_value: 117
#    mean_value: 123
#    mirror: false
#  }
  data_param {
    source: "examples/imagenet/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}

// =================================== 1 ===========================================
# 使用 LMDB 数据源 --- 一般做分类用
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}

// =================================== 2 ===========================================
# 使用 HDF5 数据源 --- 一般做回归用
layer {
  name: "data"
  type: "HDF5Data"
  top: "data"
  top: "label"
  hdf5_data_param {
    source: "examples/hdf5_classification/data/train.txt" # 在此文件写hdf5数据的路径
    batch_size: 64
  }
}

// =================================== 3 ===========================================
# 数据直接来源与图片
layer {
  name: "data"
  type: "ImageData" # 类型
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "data/ilsvrc/imagenet_mean.binaryproto"
  }
  image_data_param {
    source: "examples/_temp/file_list.txt" # 格式就是路径 标签（path/to/images/img123.jpg 1）
    batch_size: 64
    new_height: 256 # 如果设置就对图片进行 resize
    new_width: 256
  }
}
```

### 1.2 卷积层

* lr_mult：学习率的系数，可以在每一层单独设置，用来控制当前层的学习。在`solver.prototxt`设置的是基础学习率
* 如果想要某个层不更新，只需把 lr_mult都置为零。

```protobuf
layer {
  name: "conv1"
  type: "Convolution" # 表明这一层是卷积层
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1 # 学习率的参数。最终的学习率是这个数乘以solver.prototxt的base_lr。如果有两个lr_mult,则第一个表示权值的学习率，第二个表示偏置项的学习率。一般偏置项的学习率是权值学习率的两倍。
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96 # 卷积核的个数，也是生成特征图的个数
    kernel_size: 11 # 卷积核的大小
    stride: 4 # 卷积核的步长
    pad: 0 # 扩充边缘，默认0，不扩充
    weight_filler {
      type: "gaussian" # 权值初始化，默认为"constant"，值全为0，还有"xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant" # 偏置项的初始化。一般设置为 "constant"，值全为0
      value: 0
    }
  }
}
```

### 1.3 池化层

```protobuf
# pooling 层的运算方法基本和卷积层一样
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX # 池化方法，默认为MAX。目前可用的方法有MAX,AVE
    kernel_size: 3 # 池化的核大小
    stride: 2 # 池化的步长，默认为1.一般设置为2，即不重叠。
  }
}
```

### 1.4 激活函数

在激活层中，对输入数据进行激活操作，是逐元素进行运算的，在运算过程中，没有改变数据的大小，即输入和输出的数据大小是相等的。

* 一般每个卷积层后面都接一个激活层

有两种：

1. ReLu
2. Sigmoid

```protobuf
# ReLu 是目前使用的最多的激活函数，主要因为其收敛更快，并且能保持同样效果。标准的ReLu函数为max(x,0),当x>0时，# 输出x；当x<=0时，输出0。f(x)=max(x,0)

layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}

# Sigmoid

layer {
  name: "test"
  type: "Sigmoid"
  bottom: "conv"
  top: "test"
}
```

### 1.5 全连接层

```protobuf
# 全连接层，输出的是一个简单向量，参数跟卷基层一样
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
```

### 1.6 softmax

```protobuf
# softmax-loss layer : 输出 Loss 值
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}

# softmax layer :输出似然值（准确率）
layer {
  name: "prob"
  type: "Softmax"
  bottom: "cls3_fc"
  top: "prob"
}

```

### 1.7 reshape层

* 该层的作用是：在不改变数据维度的情况下，改变输入的维度。
* 我的理解是：改变某个数据的高宽。

```protobuf
layer {
    name: "reshape"
    type: "Reshape"
    bottom: "input"
    top: "output"
    reshape_param {
      shape {
        dim: 0  # copy the dimension from below
        dim: 2
        dim: 3
        dim: -1 # infer it from the other dimensions
      }
    }
  }

有一个可选的参数组shape, 用于指定blob数据的各维的值（blob是一个四维的数据：n*c*w*h）。

dim:0  表示维度不变，即输入和输出是相同的维度。

dim:2 或 dim:3 将原来的维度变成2或3

dim:-1 表示由系统自动计算维度。数据的总量不变，系统会根据blob数据的其它三维来自动计算当前维的维度值 。

假设原数据为：32*3*28*28， 表示32张3通道的28*28的彩色图片
  shape {
  dim: 0
  dim: 0
  dim: 14
  dim: -1
  }
输出数据为：32*3*14*56

#Dropout是一个防止过拟合的层
#只需要设置一个dropout_ratio就可以了。
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7-conv"
  top: "fc7-conv"
  dropout_param {
    dropout_ratio: 0.5 # 为了防止过拟合，杀死50%的神经元，让它不参与。还过拟合就设大一点。
  }
}
```

## 二、解算文件 --- solver.prototxt

作用：控制网络怎么进行训练。

```protobuf
#往往loss function是非凸的，没有解析解,我们需要通过优化方法来求解。
#caffe提供了六种优化算法来求解最优参数，在solver配置文件中，通过设置type类型来选择。

    Stochastic Gradient Descent (type: "SGD"),
    AdaDelta (type: "AdaDelta"),
    Adaptive Gradient (type: "AdaGrad"),
    Adam (type: "Adam"),
    Nesterov’s Accelerated Gradient (type: "Nesterov") and
    RMSprop (type: "RMSProp")

net: "examples/mnist/lenet_train_test.prototxt"  
test_iter: 100
test_interval: 500
base_lr: 0.01
momentum: 0.9
type: "SGD" # 视频作者笔记没有""
weight_decay: 0.0005
lr_policy: "inv"
gamma: 0.0001
power: 0.75
display: 100
max_iter: 20000
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: CPU

net: "examples/mnist/lenet_train_test.prototxt" #网络位置
train_net: "examples/hdf5_classification/logreg_auto_train.prototxt" #也可以分别设定train和test
test_net: "examples/hdf5_classification/logreg_auto_test.prototxt"

test_iter: 100 #迭代了多少个测试样本呢？ batch*test_iter 假设有5000个测试样本，一次测试想跑遍这5000个则需要设置test_iter×batch=5000

test_interval: 500 #测试间隔。也就是每训练500次，才进行一次测试。


base_lr: 0.01 #base_lr用于设置基础学习率

lr_policy: "inv" #学习率调整的策略

        - fixed:　　 保持base_lr不变.
        - step: 　　 如果设置为step,则还需要设置一个stepsize,  返回 base_lr * gamma ^ (floor(iter / stepsize)),其中iter表示当前的迭代次数
        - exp:   　　返回base_lr * gamma ^ iter， iter为当前迭代次数
        - inv:　　    如果设置为inv,还需要设置一个power, 返回base_lr * (1 + gamma * iter) ^ (- power)
        - multistep: 如果设置为multistep,则还需要设置一个stepvalue。这个参数和step很相似，step是均匀等间隔变化，而multistep则是根据stepvalue值变化
        - poly: 　　  学习率进行多项式误差, 返回 base_lr (1 - iter/max_iter) ^ (power)
        - sigmoid:　学习率进行sigmod衰减，返回 base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))

momentum ：0.9 #动量

display: 100 #每训练100次，在屏幕上显示一次。如果设置为0，则不显示。

max_iter: 20000 #最大迭代次数，2W次就停止了

snapshot: 5000 #快照。将训练出来的model和solver状态进行保存，snapshot用于设置训练多少次后进行保存
snapshot_prefix: "examples/mnist/lenet"

solver_mode: CPU #设置运行模式。默认为GPU,如果你没有GPU,则需要改成CPU,否则会出错。
```

# develop

1. 神经元
2. 神经元模型MP

    * 在神经元模型里，每个有向箭头表示的是值的加权传递
    * 一个神经元可以引出多个代表输出的有向箭头，但值都是一样的。
    * 神经元可以看作一个计算与存储单元。计算是神经元对其的输入进行计算功能。存储是神经元会暂存计算结果，并传递到下一层。
    * 当我们用“神经元”组成网络以后，描述网络中的某个“神经元”时，我们更多地会用“单元”（unit）来指代。同时由于神经网络的表现形式是一个有向图，有时也会用“节点”（node）来表达同样的意思。

  2.1 效果

    神经元模型的使用可以这样理解：
    我们有一个数据，称之为样本。样本有四个属性，其中三个属性已知，一个属性未知。
    我们需要做的就是通过三个已知属性预测未知属性。

    具体办法就是使用神经元的公式进行计算。三个已知属性的值是a1，a2，a3，
    未知属性的值是z。z可以通过公式计算出来。

    这里，已知的属性称之为**特征**,未知的属性称之为**目标**。
    假设特征与目标之间确实是线性关系，并且我们已经得到表示这个关系的权值w1，w2，w3。那么，我们就可以通过神经元模型预测新样本的目标。

  2.2 影响
    1943年发布的 MP 模型的权值是预先设置好的。不能学习。
    1949年Hebb提出Hebb学习率。才开始考虑用调整权值的方法让机器学习。

3. 单层神经网络（感知器）

    * 在原来MP模型的“输入”位置添加神经元节点，标志其为“输入单元”。其余不变
    * 输入层：输入层里的“输入单元”只负责传输数据，不做计算
    * 输出层：输出层里的“输出单元”则需要对前面一层的输入进行计算。
    * 计算层：我们把需要计算的层次称之为“计算层”，并把拥有一个计算层的网络称之为“单层神经网络”。有一些文献会按照网络拥有的层数来命名，例如把“感知器”称为两层神经网络。
    * 假如我们要预测的目标不再是一个值，而是一个向量，例如[2,3]。那么可以在输出层再增加一个“输出单元”。
    * 只能做简单的线性分类任务。对异或这样的简单分类任务无法解决。

4. 两层神经网络（多层感知器）

    * 两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，中间层和输出层都是计算层。
    * 增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。不过两层神经网络的计算是一个问题，没有一个较好的解法。
    * 提出了反向传播（Backpropagation，BP）算法，解决了两层神经网络所需要的复杂计算量问题
    * 偏置节点（bias unit）:这些节点是默认存在的。它本质上是一个只含有存储功能，且存储值永远为1的单元。在神经网络的每个层次中，除了输出层以外，都会含有这样一个偏置单元。偏置单元与后一层的所有节点都有连接，我们设这些参数值为向量b，称之为偏置。偏置节点很好认，因为其没有输入（前一层中没有箭头指向它）